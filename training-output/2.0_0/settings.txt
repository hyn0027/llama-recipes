torchrun --nnodes 1 --nproc_per_node 8  examples/finetuning.py \
    --dataset "AMR2_dataset" \
    --enable_fsdp --use_peft --peft_method lora \
    --model_name /workspace/ModelWeights/llama-2-7b-hf \
    --fsdp_config.pure_bf16 \
    --output_dir /workspace/llama-recipes/training-output/AMR2.0 \
    --num_epochs 60 --lr 1e-5 > log.txt 2>&1
